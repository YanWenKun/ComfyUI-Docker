# 基于 AMD GPU + ROCm 运行 ComfyUI

image:https://github.com/YanWenKun/ComfyUI-Docker/actions/workflows/build-rocm.yml/badge.svg["GitHub Workflow Status",link="https://github.com/YanWenKun/ComfyUI-Docker/actions/workflows/build-rocm.yml"]

https://hub.docker.com/r/yanwk/comfyui-boot/tags?name=rocm[在 <Docker Hub> 上查看]

## 注：构建镜像

本仓库会定期自动发布最新镜像到 Docker Hub，但由于 ROCm PyTorch 太大，GitHub Actions 经常报错“存储空间不足”，导致发布失败。

因此，下文中的运行命令会先在本地构建镜像（构建过程基本就是下载软件包，加上少量编译）。

但如果当前
https://hub.docker.com/r/yanwk/comfyui-boot/tags?name=rocm[Docker Hub]
上的 `rocm` 镜像发布日期较新，你也可以跳过“构建镜像”步骤以节约时间。

## 准备工作

* 确保 Linux 宿主机上正确安装了
https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-radeon.html[Radeon software for Linux with ROCm]。

## 构建并运行

需要根据 GPU 添加如下环境变量（感谢
https://github.com/YanWenKun/ComfyUI-Docker/pull/67[nhtua]
的 PR）：

* RDNA 2 :
** `-e HSA_OVERRIDE_GFX_VERSION=10.3.0 \`

* RDNA 3 :
** `-e HSA_OVERRIDE_GFX_VERSION=11.0.0 \`
** 参看 https://rocm.docs.amd.com/en/latest/reference/gpu-arch-specs.html[AMD文档] ，部分型号可改用 `11.0.1`

* RDNA 4 :
** `-e HSA_OVERRIDE_GFX_VERSION=12.0.0 \`
** 参看 https://rocm.docs.amd.com/en/latest/reference/gpu-arch-specs.html[AMD文档] ，部分型号可改用 `12.0.1`

* APU／集显 :
** `-e HIP_VISIBLE_DEVICES=0 \`

额外还可添加环境变量：

* 启用可调优操作（首次运行较慢，但之后运行更快，
https://github.com/ROCm/pytorch/tree/main/aten/src/ATen/cuda/tunable[文档1] ，
https://github.com/Comfy-Org/docs/blob/main/troubleshooting/overview.mdx#amd-gpu-issues[文档2] 。
感谢
https://github.com/YanWenKun/ComfyUI-Docker/pull/114[SergeyFilippov]
的 PR）：

** `-e PYTORCH_TUNABLEOP_ENABLED=1 \`

.使用 Docker
[source,sh]
----
# 构建镜像
git clone https://github.com/YanWenKun/ComfyUI-Docker.git
cd ComfyUI-Docker/rocm
docker build . -t yanwk/comfyui-boot:rocm

# 运行容器
mkdir -p storage \
  storage-models/models \
  storage-models/hf-hub \
  storage-models/torch-hub \
  storage-user/input \
  storage-user/output \
  storage-user/workflows

docker run -it --rm \
  --name comfyui-rocm \
  --device=/dev/kfd --device=/dev/dri \
  --group-add=video --ipc=host --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --security-opt label=disable \
  -p 8188:8188 \
  -v "$(pwd)"/storage:/root \
  -v "$(pwd)"/storage-models/models:/root/ComfyUI/models \
  -v "$(pwd)"/storage-models/hf-hub:/root/.cache/huggingface/hub \
  -v "$(pwd)"/storage-models/torch-hub:/root/.cache/torch/hub \
  -v "$(pwd)"/storage-user/input:/root/ComfyUI/input \
  -v "$(pwd)"/storage-user/output:/root/ComfyUI/output \
  -v "$(pwd)"/storage-user/workflows:/root/ComfyUI/user/default/workflows \
  -e CLI_ARGS="" \
  yanwk/comfyui-boot:rocm
----

.使用 Podman
[source,sh]
----
# 构建镜像
git clone https://github.com/YanWenKun/ComfyUI-Docker.git
cd ComfyUI-Docker/rocm
podman build . -t yanwk/comfyui-boot:rocm

# 运行容器
mkdir -p storage \
  storage-models/models \
  storage-models/hf-hub \
  storage-models/torch-hub \
  storage-user/input \
  storage-user/output \
  storage-user/workflows

podman run -it --rm \
  --name comfyui-rocm \
  --device=/dev/kfd --device=/dev/dri \
  --group-add=video --ipc=host --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --security-opt label=disable \
  -p 8188:8188 \
  -v "$(pwd)"/storage:/root \
  -v "$(pwd)"/storage-models/models:/root/ComfyUI/models \
  -v "$(pwd)"/storage-models/hf-hub:/root/.cache/huggingface/hub \
  -v "$(pwd)"/storage-models/torch-hub:/root/.cache/torch/hub \
  -v "$(pwd)"/storage-user/input:/root/ComfyUI/input \
  -v "$(pwd)"/storage-user/output:/root/ComfyUI/output \
  -v "$(pwd)"/storage-user/workflows:/root/ComfyUI/user/default/workflows \
  -e CLI_ARGS="" \
  yanwk/comfyui-boot:rocm
----

启动完成后，访问 http://localhost:8188/

[[hint]]
## 如果你愿意折腾……

（以下内容介绍另外的安装使用方式，与本镜像无关）

使用由 AMD 构建的 ROCm PyTorch 镜像：

https://hub.docker.com/r/rocm/pytorch

这个镜像很大，但如果你运行容器遇到困难，可以尝试用这个镜像手动安装运行 ComfyUI。
它已经安装好了最重要的 PyTorch，你只需要再安装少量 Python 包即可运行 ComfyUI。

[source,sh]
----
docker pull rocm/pytorch:rocm7.0.2_ubuntu24.04_py3.12_pytorch_release_2.8.0

mkdir -p storage

docker run -it --rm \
  --name comfyui-rocm \
  --device=/dev/kfd --device=/dev/dri \
  --group-add=video --ipc=host --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --security-opt label=disable \
  -p 8188:8188 \
  --user root \
  --workdir /root/workdir \
  -v "$(pwd)"/storage:/root/workdir \
  rocm/pytorch:rocm7.0.2_ubuntu24.04_py3.12_pytorch_release_2.8.0 \
  /bin/bash

git clone https://github.com/comfyanonymous/ComfyUI.git

pip install -r ComfyUI/requirements.txt
# 或使用 Conda:
# conda install --yes --file ComfyUI/requirements.txt

python ComfyUI/main.py --listen --port 8188
# 或使用 python3:
# python3 ComfyUI/main.py --listen --port 8188
----

## 备注： Windows 用户

（以下内容介绍另外的安装使用方式，与本镜像无关）

WSL2 支持 ROCm 与 DirectML：

* ROCm

** 如果你的 AMD GPU 在
https://rocm.docs.amd.com/projects/radeon/en/latest/docs/compatibility/wsl/wsl_compatibility.html[兼容性列表]
中，你可以在 WSL2 环境中安装
https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/wsl/install-radeon.html[Radeon software]
，也可以通过 Docker Desktop 使用
<<hint, ROCm PyTorch 镜像>>。

* DirectML

** DirectML 支持大多数 GPU（包括 AMD APU 与 Intel GPU）。
该方法比纯 CPU 快，比 Linux 下的 ROCm 慢，且支持的 GPU 型号更多（甚至核显也能跑）。

** 见：
link:../docs/wsl-directml.zh.adoc[在 WSL2 环境下通过 DirectML 运行 ComfyUI]。

* ZLUDA

** 这里 ZLUDA 不是跑在 WSL2 上，而是 Windows 原生运行。ZLUDA 能“翻译”CUDA 指令给 AMD GPU 运行。
这里不写详细了，因为老方法很可能一更新就不能用了，还请搜索教程。
但还是提一点建议，先试着跑 SD-WebUI，这个起手要容易不少。
