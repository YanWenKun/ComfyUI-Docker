# Docker images for ComfyUI

*link:README.zh.adoc[＞💡中文说明点我💡＜]*

This repo is for 
https://hub.docker.com/r/yanwk/comfyui-boot[Docker images] 
that runs 
https://github.com/comfyanonymous/ComfyUI[ComfyUI] - 
a Stable Diffusion GUI powering node-based workflow.

## Quick Start - NVIDIA GPU

```sh
mkdir -p storage
mkdir -p storage-models/models
mkdir -p storage-models/hf-hub
mkdir -p storage-models/torch-hub
mkdir -p storage-user/input
mkdir -p storage-user/output
mkdir -p storage-user/workflows

docker run -it --rm \
  --name comfyui \
  --gpus all \
  -p 8188:8188 \
  -v "$(pwd)"/storage:/root \
  -v "$(pwd)"/storage-models/models:/root/ComfyUI/models \
  -v "$(pwd)"/storage-models/hf-hub:/root/.cache/huggingface/hub \
  -v "$(pwd)"/storage-models/torch-hub:/root/.cache/torch/hub \
  -v "$(pwd)"/storage-user/input:/root/ComfyUI/input \
  -v "$(pwd)"/storage-user/output:/root/ComfyUI/output \
  -v "$(pwd)"/storage-user/workflows:/root/ComfyUI/user/default/workflows \
  -e CLI_ARGS="--disable-xformers" \
  yanwk/comfyui-boot:cu128-slim
```

## Support Matrix

The supported CUDA versions for each GPU architecture are shown in the table below:

[cols="1,1,1,1,1,1,1,1,1", options="header"]
|===
| GPU Architecture
| Blackwell | Hopper | Ada Lovelace | Ampere | Turing | Volta | Pascal | Maxwell

| Example GPU
| RTX 5090 | H100 | RTX 4090 | RTX 3090 
| RTX 2080 +
GTX 1660 
| TITAN V | GTX 1080 | GTX 980

| cu130
| ✔️ | ✔️ | ✔️ | ✔️ | ✔️ | ❌ | ❌ | ❌

| cu129
| ✔️ | ✔️ | ✔️ | ✔️ | ✔️ | ❌ | ❌ | ❌

| cu128 ⭐
| ✔️ | ✔️ | ✔️ | ✔️ | ✔️ | ✔️ | ❌ | ❌

| cu126
| ❌ | ✔️ | ✔️ | ✔️ | ✔️ | ✔️ | ✔️ | ✔️

|===

* CUDA 12.8 images are currently recommended, in line with PyTorch’s 
https://github.com/pytorch/pytorch/issues/159980[development plan].

* CUDA 12.9 images will remain on PyTorch 2.8.0 and will be discontinued, as PyTorch does not plan to build 2.9.0+cu129.

* CUDA 13.0 images are experimental, using PyTorch 2.9 RC and Python 3.14 RC.

* __If you are unsure about your NVIDIA GPU architecture, see
https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/[this article].__

* __Note: These CUDA compatibility limitations are due to the
https://github.com/pytorch/pytorch/releases/tag/v2.8.0[PyTorch] toolchain,
not the NVIDIA CUDA Toolkit.
For more information, refer to the 
https://github.com/pytorch/pytorch/blob/main/.ci/manywheel/build_cuda.sh[PyTorch build script].__


## More image tags

* link:rocm/README.adoc[`rocm`]

** For AMD GPUs with ROCm.

* link:xpu/[`xpu`]

** For Intel GPUs with XPU.

* link:nightly/README.adoc[`nightly`]

** Using preview version of PyTorch (CUDA).

* link:cu121/README.adoc[`cu121`]

** Easy for ComfyUI beginners. Starts with ComfyUI, ComfyUI-Manager and the Photon (SD1.5) model.
** Using a low-privilege user within the container (easy for WSL2 deploy).
** Not recommended for Podman or rootless deploy (use any image below instead).
** Using CUDA 12.1 + Python 3.11.

* link:cu124-slim/README.adoc[`cu124-slim`]

** Similar to `cu121`, equipped with many dependencies, starts with ComfyUI and ComfyUI-Manager only.
** Downloads less. No SD model included.
** Using 'root' user within the container (easy for rootless deploy).
** Using CUDA 12.4 + Python 3.12.

* link:cu121-megapak/README.adoc[`cu121-megapak`]

** All-in-one bundle, including dev kits.
** Using CUDA 12.1 + Python 3.11.

* link:cu124-megapak/README.adoc[`cu124-megapak`]

** All-in-one bundle, including dev kits.
** Using CUDA 12.4 + Python 3.12.

* link:cu124-cn/README.adoc[`cu124-cn`]

** For users in mainland China. Using mirror sites for all download links.

* link:comfy3d-pt25/README.adoc[`comfy3d-pt25`]

** Image dedicated for https://github.com/MrForExample/ComfyUI-3D-Pack[ComfyUI-3D-Pack].


## License

link:LICENSE[Mulan Public License，Version 2]

This open source license is written and valid both in Chinese and English, how good is that!
